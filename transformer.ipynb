{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145a866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7dc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = [[\"salut\",\"comment\",\"ca\",\"va\",\"?\"]]\n",
    "output_embedding = [['<START>','Hi','how','are','you','?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206a7e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD>': 7}\n",
      "{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 6, '<PAD>': 7}\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(sequences):\n",
    "    token_to_info = {}\n",
    "    for sequence  in sequences:\n",
    "        for word in sequence:\n",
    "            if word not in token_to_info:\n",
    "                token_to_info[word] = len(token_to_info)\n",
    "\n",
    "    return token_to_info\n",
    "\n",
    "input_vocabulary = get_vocabulary(input_embedding)\n",
    "output_vocabulary = get_vocabulary(output_embedding)\n",
    "\n",
    "input_vocabulary['<START>'] = len(input_vocabulary)\n",
    "input_vocabulary['<END>'] = len(input_vocabulary)\n",
    "input_vocabulary['<PAD>'] = len(input_vocabulary)\n",
    "\n",
    "\n",
    "output_vocabulary['<END>'] = len(output_vocabulary)\n",
    "output_vocabulary['<PAD>'] = len(output_vocabulary)\n",
    "\n",
    "print(input_vocabulary)\n",
    "print(output_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd193de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]]\n",
      "[[0 1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_int(sequences ,voc):\n",
    "    seqs_int = []\n",
    "    for sequence in sequences:\n",
    "        seq_int =[]\n",
    "        for word in sequence:\n",
    "            seq_int.append(voc[word])\n",
    "        seqs_int.append(seq_int)\n",
    "    return np.array(seqs_int)\n",
    "\n",
    "input_seq = sequences_to_int(input_embedding,input_vocabulary)\n",
    "output_seq = sequences_to_int(output_embedding,output_vocabulary)\n",
    "\n",
    "print(input_seq)\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1733c0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ scaled_dot_attention_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ScaledDotAttention</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ scaled_dot_attention_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mScaledDotAttention\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self ,nb_token,**kwargs ):\n",
    "        self.nb_token = nb_token\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self ,input_shape):\n",
    "        self.word_embedding = tf.keras.layers.Embedding(\n",
    "            self.nb_token,\n",
    "            256\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self ,x):\n",
    "        embed = self.word_embedding(x)\n",
    "        return embed\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class ScaledDotAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self ,**kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self ,input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self ,x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5,1))\n",
    "\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    attention = ScaledDotAttention()(layer_input)\n",
    "    model = tf.keras.Model(layer_input,attention)\n",
    "    model.summary()\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c895e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
